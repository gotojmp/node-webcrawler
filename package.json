{
  "name": "node-webcrawler",
  "version": "0.7.12",
  "description": "Crawler is a web spider written with Nodejs. It gives you the full power of jQuery on the server to parse a big number of pages as they are downloaded, asynchronously",
  "main": "./lib/crawler.js",
  "directories": {
    "test": "tests"
  },
  "scripts": {
    "test": "./node_modules/mocha/bin/mocha --reporter spec --bail --timeout 10000 tests/*.js"
  },
  "files": [
    "lib/"
  ],
  "repository": {
    "type": "git",
    "url": "https://github.com/gotojmp/node-webcrawler.git"
  },
  "dependencies": {
    "charset-parser": "^0.2.0",
    "cheerio": "0.22.0",
    "generic-pool": "2.2.0",
    "iconv": "2.1.7",
    "iconv-lite": "0.4.8",
    "lodash": "3.8.0",
    "request": "2.75.0",
    "seenreq": "^0.1.7",
    "bottleneck": "1.12.0"
  },
  "optionalDependencies": {
    "iconv": "*"
  },
  "devDependencies": {
    "chai": "2.3.0",
    "mocha": "2.2.5",
    "mocha-testdata": "1.1.0",
    "sinon": "1.14.1",
    "jsdom": "3.1.2"
  },
  "keywords": [
    "dom",
    "javascript",
    "crawling",
    "spider",
    "scraper",
    "scraping",
    "jquery",
    "webcrawler",
    "crawler"
  ],
  "author": "gotojmp",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/gotojmp/node-webcrawler/issues"
  },
  "homepage": "https://github.com/gotojmp/node-webcrawler"
}
